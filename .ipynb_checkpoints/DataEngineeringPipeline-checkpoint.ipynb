{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bc68ac-8810-4e13-bb7b-938651eb7e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (37.6.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from faker) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64db2ad2-209c-4055-9518-0b9052ffe923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5704c3-1c76-4601-9b26-ddc65f629b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e78a80b-c9e7-4d85-9944-790bbafe5ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: faker in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (37.6.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (2.3.2)\n",
      "Collecting sqlalchemy\n",
      "  Using cached sqlalchemy-2.0.43-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Requirement already satisfied: tzdata in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from faker) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Using cached greenlet-3.2.4-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mind-graph\\tasks\\data engineerng pipeline\\dep\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached sqlalchemy-2.0.43-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Using cached greenlet-3.2.4-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "\n",
      "   ---------------------------------------- 0/2 [greenlet]\n",
      "   ---------------------------------------- 0/2 [greenlet]\n",
      "   ---------------------------------------- 0/2 [greenlet]\n",
      "   ---------------------------------------- 0/2 [greenlet]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   -------------------- ------------------- 1/2 [sqlalchemy]\n",
      "   ---------------------------------------- 2/2 [sqlalchemy]\n",
      "\n",
      "Successfully installed greenlet-3.2.4 sqlalchemy-2.0.43\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark faker pyyaml pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da1fa0eb-e835-4b8c-a232-def0b52d7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql.functions import col,when,coalesce,lit,sum as _sum,count,month,year,round\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2f7d5e-0552-49c8-8869-823c1c058946",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca5b32c-e6f4-40e6-986d-0af29dcb40e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    customer_id                  name                         email  \\\n",
      "0             1        Kristen Wright       kristenwright@gmail.com   \n",
      "1             2        Jessica Carter       jessicacarter@gmail.com   \n",
      "2             3      Michael Reynolds     michaelreynolds@gmail.com   \n",
      "3             4            Tracy Hale           tracyhale@gmail.com   \n",
      "4             5         Shaun Delgado        shaundelgado@gmail.com   \n",
      "5             6       Shawn Hernandez      shawnhernandez@gmail.com   \n",
      "6             7         Timothy Lewis        timothylewis@gmail.com   \n",
      "7             8           John Barker          johnbarker@gmail.com   \n",
      "8             9        Joel Robertson       joelrobertson@gmail.com   \n",
      "9            10         Whitney Morse                          None   \n",
      "10           11            Terri Bray           terribray@gmail.com   \n",
      "11           12        Ashley Collins       ashleycollins@gmail.com   \n",
      "12           13          Jessica Hill         jessicahill@gmail.com   \n",
      "13           14           Adam Watson          adamwatson@gmail.com   \n",
      "14           15         Cynthia Moore                          None   \n",
      "15           16         Joshua Weaver        joshuaweaver@gmail.com   \n",
      "16           17      Jessica Campbell     jessicacampbell@gmail.com   \n",
      "17           18     Michael Rodriguez    michaelrodriguez@gmail.com   \n",
      "18           19      Michael Jacobson     michaeljacobson@gmail.com   \n",
      "19           20         ###INVALID###                          None   \n",
      "20           21        Carolyn Arnold       carolynarnold@gmail.com   \n",
      "21           22     Anthony Rodriguez    anthonyrodriguez@gmail.com   \n",
      "22           23          Amy Lawrence         amylawrence@gmail.com   \n",
      "23           24          Cathy Mclean         cathymclean@gmail.com   \n",
      "24           25         Justin Sawyer        justinsawyer@gmail.com   \n",
      "25           26          Dana Stewart         danastewart@gmail.com   \n",
      "26           27         Derrick Davis        derrickdavis@gmail.com   \n",
      "27           28        Stephen Watson       stephenwatson@gmail.com   \n",
      "28           29  Joshua Gutierrez DDS  joshuagutierrezdds@gmail.com   \n",
      "29           30      Lindsay Cain DVM                          None   \n",
      "\n",
      "                                         country  \n",
      "0   South Georgia and the South Sandwich Islands  \n",
      "1                                          Congo  \n",
      "2                                          Yemen  \n",
      "3                                       Botswana  \n",
      "4                                       Ethiopia  \n",
      "5                                        Algeria  \n",
      "6                                          Macao  \n",
      "7                                          Nauru  \n",
      "8                                        Georgia  \n",
      "9                                     Micronesia  \n",
      "10                                     Greenland  \n",
      "11                                          Peru  \n",
      "12                              Marshall Islands  \n",
      "13                                         Ghana  \n",
      "14                                        Guyana  \n",
      "15                                        Malawi  \n",
      "16                                 Guinea-Bissau  \n",
      "17                                     Greenland  \n",
      "18                                        Jersey  \n",
      "19                                    Martinique  \n",
      "20             Heard Island and McDonald Islands  \n",
      "21                                     Hong Kong  \n",
      "22                     Saint Pierre and Miquelon  \n",
      "23                                        Kuwait  \n",
      "24                                        Greece  \n",
      "25                                         Korea  \n",
      "26                                       Tunisia  \n",
      "27                                        Mexico  \n",
      "28                                          Guam  \n",
      "29                                       Belgium  \n"
     ]
    }
   ],
   "source": [
    "customers = []\n",
    "for i in range(1,71):\n",
    "    name=fake.name()\n",
    "    email_name=name.lower().replace(\" \",\"\")\n",
    "    email=f\"{email_name}@gmail.com\"\n",
    "    country=fake.country()\n",
    "    if i%10==0:\n",
    "      email=None\n",
    "    if i%15==0:\n",
    "      email=None\n",
    "    if i%20==0:\n",
    "      name=\"###INVALID###\"\n",
    "\n",
    "    customers.append({\n",
    "        \"customer_id\": i,\n",
    "        \"name\": name,\n",
    "        \"email\": email,\n",
    "        \"country\":country\n",
    "    })\n",
    "pd.DataFrame(customers).to_csv(f\"{out_dir }/customers.csv\", index=False)\n",
    "customers_df=pd.DataFrame(customers)\n",
    "print(customers_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44717d71-8029-4f29-ae5e-9103e40c7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  order_date   amount     status\n",
      "0         1           47          46  2025-09-08  1699.59       None\n",
      "1         2           16          10  2025-09-08  1624.16    INVALID\n",
      "2         3            9          32  2025-09-08   155.37    INVALID\n",
      "3         4           63          10  2025-09-08  1629.32  cancelled\n",
      "4         5           16          36  2025-09-08   796.44    INVALID\n"
     ]
    }
   ],
   "source": [
    "statuses = [\"completed\", \"pending\", \"cancelled\", \"returned\", None, \"INVALID\"]\n",
    "orders = []\n",
    "for i in range(1, 100):\n",
    "    order_date = fake.date_between(start_date='-6m', end_date='today').strftime(\"%Y-%m-%d\")\n",
    "    if i%17==0:\n",
    "      order_date=None\n",
    "    amount=round(random.uniform(-50, 2000), 2)\n",
    "    if i%12==0:\n",
    "      amount=-abs(amount)\n",
    "    status=random.choice(statuses)\n",
    "    orders.append({\n",
    "        \"order_id\": i,\n",
    "        \"customer_id\": random.randint(1, 70),\n",
    "        \"product_id\": random.randint(1, 50),\n",
    "        \"order_date\": order_date,\n",
    "        \"amount\": amount,\n",
    "        \"status\": status\n",
    "    })\n",
    "pd.DataFrame(orders).to_csv(f\"{out_dir}/orders.csv\", index=False)\n",
    "orders_df=pd.DataFrame(orders)\n",
    "print(orders_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87463030-7299-47f9-9bb5-783e58a37558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id                        name     category    price\n",
      "0           1                   iPhone 14  Electronics  1108.34\n",
      "1           2          Samsung Galaxy S23  Electronics   383.79\n",
      "2           3              MacBook Pro 14  Electronics  1179.71\n",
      "3           4  Sony WH-1000XM5 Headphones  Electronics   705.12\n",
      "4           5        Apple Watch Series 9    Wearables   280.76\n"
     ]
    }
   ],
   "source": [
    "products_catalog = [\n",
    "    (\"iPhone 14\", \"Electronics\"), (\"Samsung Galaxy S23\", \"Electronics\"),\n",
    "    (\"MacBook Pro 14\", \"Electronics\"), (\"Sony WH-1000XM5 Headphones\", \"Electronics\"),\n",
    "    (\"Apple Watch Series 9\", \"Wearables\"), (\"Fitbit Charge 5\", \"Wearables\"),\n",
    "    (\"Dell XPS 13 Laptop\", \"Electronics\"), (\"iPad Air\", \"Tablets\"),\n",
    "    (\"Bose QuietComfort Earbuds\", \"Audio\"), (\"Samsung Galaxy Buds\", \"Audio\"),\n",
    "    (\"Canon EOS Rebel Camera\", \"Photography\"), (\"GoPro HERO10\", \"Photography\"),\n",
    "    (\"Nike Air Max Shoes\", \"Footwear\"), (\"Puma Running Shoes\", \"Footwear\"),\n",
    "    (\"Levi's 501 Jeans\", \"Apparel\"), (\"Adidas Hoodie\", \"Apparel\"),\n",
    "    (\"Zara T-Shirt\", \"Apparel\"), (\"H&M Casual Dress\", \"Apparel\"),\n",
    "    (\"Gucci Leather Belt\", \"Accessories\"), (\"Under Armour Shorts\", \"Sportswear\"),\n",
    "    (\"Harry Potter Book Set\", \"Books\"), (\"The Lord of the Rings Trilogy\", \"Books\"),\n",
    "    (\"Atomic Habits\", \"Books\"), (\"Rich Dad Poor Dad\", \"Books\"),\n",
    "    (\"Python Crash Course\", \"Books\"), (\"Clean Code\", \"Books\"),\n",
    "    (\"Game of Thrones Box Set\", \"Books\"), (\"The Lean Startup\", \"Books\"),\n",
    "    (\"To Kill a Mockingbird\", \"Books\"), (\"1984\", \"Books\"),\n",
    "    (\"Ikea Dining Table\", \"Furniture\"), (\"Sealy Memory Foam Mattress\", \"Furniture\"),\n",
    "    (\"Ikea Chair Set\", \"Furniture\"), (\"Philips Air Fryer\", \"Kitchen Appliances\"),\n",
    "    (\"NutriBullet Blender\", \"Kitchen Appliances\"), (\"Instant Pot Cooker\", \"Kitchen Appliances\"),\n",
    "    (\"Hamilton Beach Toaster\", \"Kitchen Appliances\"), (\"Keurig Coffee Maker\", \"Kitchen Appliances\"),\n",
    "    (\"Philips LED Desk Lamp\", \"Lighting\"), (\"Dyson V15 Vacuum Cleaner\", \"Home Appliances\"),\n",
    "    (\"Wilson Tennis Racket\", \"Sports Equipment\"), (\"Adidas Football\", \"Sports Equipment\"),\n",
    "    (\"Spalding Basketball\", \"Sports Equipment\"), (\"Yonex Badminton Racket\", \"Sports Equipment\"),\n",
    "    (\"Nike Yoga Mat\", \"Fitness\"), (\"Reebok Jump Rope\", \"Fitness\"),\n",
    "    (\"Speedo Swimming Goggles\", \"Sports Accessories\"), (\"Titleist Golf Balls\", \"Sports Equipment\"),\n",
    "    (\"LEGO Star Wars Set\", \"Toys & Games\"), (\"Barbie Dreamhouse\", \"Dolls\"),\n",
    "    (\"Hot Wheels Track Set\", \"Toys & Games\"), (\"Monopoly Board Game\", \"Board Games\"),\n",
    "    (\"Rubik's Cube\", \"Puzzles\"), (\"Nerf Elite Blaster\", \"Outdoor Toys\"),\n",
    "    (\"Play-Doh Fun Pack\", \"Arts & Crafts\"), (\"Fisher-Price Baby Gym\", \"Baby Toys\"),\n",
    "    (\"Disney Princess Doll\", \"Dolls\"), (\"LEGO Technic Car\", \"Toys & Games\")\n",
    "]\n",
    "products = []\n",
    "for i, (name, category) in enumerate(products_catalog, start=1):\n",
    "    products.append({\n",
    "        \"product_id\": i,\n",
    "        \"name\": name,\n",
    "        \"category\": category,\n",
    "        \"price\": round(random.uniform(10, 1200), 2)\n",
    "    })\n",
    "pd.DataFrame(products).to_csv(f\"{out_dir}/products.csv\", index=False)\n",
    "products_df=pd.DataFrame(products)\n",
    "print(products_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32f92b66-b83a-42c5-bed1-14e2d0675ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11015c8e-d43b-4ee5-b1e5-6a94cc6cd5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m spark=\u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDataEngineeringPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Tasks\\Data Engineerng Pipeline\\DEP\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Tasks\\Data Engineerng Pipeline\\DEP\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Tasks\\Data Engineerng Pipeline\\DEP\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Tasks\\Data Engineerng Pipeline\\DEP\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Tasks\\Data Engineerng Pipeline\\DEP\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "customers_df=spark.read.csv(\"customers.csv\",header=True,inferSchema=True)\n",
    "orders_df=spark.read.csv(\"orders.csv\",heder=True,inferSchema=True)\n",
    "products_df=spark.read.csv(\"products.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fed5cb-7b60-429e-84e2-ee20f2cb009f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008212a9-203b-43c4-a014-3aaa34c6d6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KAERNEL",
   "language": "python",
   "name": "pyspark_kaernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
